{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Vijay Kumar! As a data scientist, you must be involved in some fascinating projects, working with data to extract insights and drive business decisions. What kind of data science work do you do, and what industries or domains are you most interested in?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 51, 'total_tokens': 107, 'completion_time': 0.203636364, 'prompt_time': 0.009969859, 'queue_time': 2.427624405, 'total_time': 0.213606223}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4e32347616', 'finish_reason': 'stop', 'logprobs': None}, id='run-349c0f50-c82a-48ac-9af5-cce3b94296b8-0', usage_metadata={'input_tokens': 51, 'output_tokens': 56, 'total_tokens': 107})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API key and URL from environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model_name = \"llama-3.3-70b-versatile\"  # Define the model name\n",
    "\n",
    "model=ChatGroq(model=model_name, groq_api_key=groq_api_key)\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi, my name is vijay kumar, i am a data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Vijay Kumar! As a data scientist, you must be working with complex data sets, machine learning algorithms, and statistical models to extract insights and make informed decisions. That's a fascinating field!\\n\\nWhat kind of projects do you usually work on? Are you more focused on predictive modeling, data visualization, or perhaps natural language processing? I'm curious to know more about your work!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 51, 'total_tokens': 133, 'completion_time': 0.298181818, 'prompt_time': 0.009105684, 'queue_time': 0.022236494, 'total_time': 0.307287502}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_fcc3b74982', 'finish_reason': 'stop', 'logprobs': None}, id='run-227bf101-86e1-4ac9-9bdf-69c579b03d64-0', usage_metadata={'input_tokens': 51, 'output_tokens': 82, 'total_tokens': 133})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, my name is vijay kumar, i am a data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Vijay Kumar, and you are a Data Scientist.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 148, 'total_tokens': 163, 'completion_time': 0.071527157, 'prompt_time': 0.036051482, 'queue_time': 0.204839926, 'total_time': 0.107578639}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3884478861', 'finish_reason': 'stop', 'logprobs': None}, id='run-1f3caca8-b7c8-4e28-a703-219dc497a979-0', usage_metadata={'input_tokens': 148, 'output_tokens': 15, 'total_tokens': 163})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi, my name is vijay kumar, i am a data scientist\"),\n",
    "    AIMessage(content=\"Nice to meet you, Vijay Kumar! As a data scientist, you must be working with data every day, extracting insights, and making informed decisions. That's a fascinating field!\\n\\nWhat kind of projects do you usually work on? Are you more focused on machine learning, predictive modeling, data visualization, or something else? I'm curious to know more about your work!\"),\n",
    "    HumanMessage(content=\"Hey, What is my name and what do i do ?\")\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "\n",
    "    Message history in Large Language Models (LLMs) or chatbots refers to the stored sequence of previous interactions between the user and the AI. This history helps the model maintain context and generate relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    \n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is vijay kumar, i am a data scientist\")],\n",
    "    config=config   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Vijay Kumar! As a data scientist, you must be working with data to extract insights and knowledge. That's a fascinating field, and I'm sure you have a lot of interesting projects and experiences to share.\\n\\nWhat kind of data science work do you do, Vijay? Are you working in a specific industry, such as finance, healthcare, or e-commerce? Or are you working on more general projects, like machine learning or natural language processing? I'm curious to know more about your work!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Vijay Kumar, and you are a Data Scientist.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, what's my name and what do i do for living\")],\n",
    "    config=config   \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Vijay Kumar.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, what's my name\")],\n",
    "    config=config1   \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know your name. I'm a large language model, I don't have the ability to retain personal information about individuals, including their names. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. Would you like to tell me your name?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, what's my name\")],\n",
    "    config=config1   \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Vijay! It's nice to meet you. Is there something I can help you with or would you like to chat? I'm here to assist you with any questions or topics you'd like to discuss. How's your day going so far?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is vijay\")],\n",
    "    config=config1   \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I remember, your name is Vijay! We just chatted about it a moment ago.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, what is my name\")],\n",
    "    config=config1   \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "\n",
    "    A prompt template is essentially a predefined structure or pattern used to guide the creation of prompts for language models (like GPT, for example). It helps ensure that your prompts are clear, specific, and generate the desired responses.\n",
    "\n",
    " Why Use Prompt Templates?\n",
    "\n",
    " Consistency: Helps maintain a standard format for all your prompts.\n",
    " \n",
    " Efficiency: Saves time when crafting prompts.\n",
    " \n",
    " Clarity: Ensures that the language model understands exactly what you're asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant, answer all the questions to the best of your knowledge and convey it very simply and clearly and be respectful\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Vijay, it's nice to meet you. How can I assist you today? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 69, 'total_tokens': 103, 'completion_time': 0.163854768, 'prompt_time': 0.015647235, 'queue_time': 0.018444064, 'total_time': 0.179502003}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3884478861', 'finish_reason': 'stop', 'logprobs': None}, id='run-96c182f1-43fe-4cd8-9f64-9237b41fadbf-0', usage_metadata={'input_tokens': 69, 'output_tokens': 34, 'total_tokens': 103})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi, my name is vijay\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Vijay, it's nice to meet you. How can I assist you today? Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke([HumanMessage(content=\"Hi, my name is vijay\")],\n",
    "                                      config=config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Vijay. We just introduced ourselves a moment ago, remember?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke([HumanMessage(content=\"Hi, what is my name\")],\n",
    "                                      config=config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOre complex prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    (\"system\", \n",
    "     \"You are a helpful assistant, answer all the questions to the best of your knowledge and convey it respectful in language {Language}\"\n",
    "     ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§µ‡§ø‡§ú‡§Ø ‡§ú‡•Ä, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi, my name is vijay\")], \"Language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lets now wrap the more complicated chain in a message history class. because there are multiple keys in the input. we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á ‡∞µ‡∞ø‡∞ú‡∞Ø‡±ç ‡∞ï‡±Å‡∞Æ‡∞æ‡∞∞‡±ç ‡∞ï‡±ä‡∞°‡∞æ‡∞Æ‡±ç, ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞è‡∞Æ‡±à‡∞®‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞æ? ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞è ‡∞µ‡∞ø‡∞∑‡∞Ø‡∞Ç‡∞™‡±à ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞ï‡±ã‡∞∞‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å? ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config4={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, my name is Vijay Kumar Kodam\")],\"Language\":\"Telugu\"},\n",
    "                                      config=config4)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á, ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞µ‡∞ø‡∞ú‡∞Ø‡±ç ‡∞ï‡±Å‡∞Æ‡∞æ‡∞∞‡±ç ‡∞ï‡±ä‡∞°‡∞æ‡∞Æ‡±ç. ‡∞Æ‡∞æ ‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø ‡∞Æ‡∞æ‡∞ü‡∞≤‡±ç‡∞≤‡±ã ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞®‡∞æ‡∞ï‡±Å ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å ‡∞ï‡∞¶‡∞æ!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config4={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, what is my name\")],\"Language\":\"Telugu\"},\n",
    "                                      config=config4)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Conversation History in LLMs (Chatbots)\n",
    "\n",
    "    Conversation history in Large Language Models (LLMs) or chatbots refers to the past messages exchanged between the user and the model. Managing this history effectively is crucial for context retention, performance optimization, and user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Generative AI Projects\\CAG in Chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like Pizza', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='Super', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 2+2', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks!', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='No Problem', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How are you doing', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='Fine', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on='human')\n",
    "\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"you are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi!, I am Vijay\"),\n",
    "    SystemMessage(content=\"Hi\"),\n",
    "    HumanMessage(content=\"I like Pizza\"),\n",
    "    SystemMessage(content=\"Super\"),\n",
    "    HumanMessage(content=\"What is 2+2\"),\n",
    "    SystemMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks!\"),\n",
    "    SystemMessage(content=\"No Problem\"),\n",
    "    HumanMessage(content=\"How are you doing\"),   \n",
    "    SystemMessage(content=\"Fine\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á! ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞§‡±Ü‡∞≤‡∞ø‡∞Ø‡∞¶‡±Å, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞®‡∞æ‡∞ï‡±Å ‡∞§‡±Ü‡∞≤‡∞ø‡∞Ø‡∞ú‡±á‡∞Ø‡∞≤‡±á‡∞¶‡±Å. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±á, ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞®‡±Å!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "       |prompt\n",
    "       |model\n",
    "       \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"what is my name\")],\"Language\":\"Telugu\"\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§®‡•á 2+2 ‡§ï‡§æ ‡§ó‡§£‡§ø‡§§ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§õ‡§æ ‡§•‡§æ‡•§'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"what math problem did i ask\")],\"Language\":\"Hindi\"\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§®‡•á ‡§Æ‡•Å‡§ù‡§∏‡•á 2+2 ‡§ï‡§æ ‡§Æ‡§æ‡§• ‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡§æ ‡§•‡§æ‡•§'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"Language\":\"Hindi\",\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document\n",
    "\n",
    "In LangChain, a document is a core concept that represents a piece of text along with metadata. It is used for processing and retrieval in AI applications, like chatbots, search engines, or knowledge bases.\n",
    "\n",
    "A LangChain Document consists of:\n",
    "\n",
    "- Content/Text ‚Üí The actual information (e.g., a paragraph, article, or structured data).\n",
    "- Metadata ‚Üí Extra information like source, date, author, or category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'official_docs', 'category': 'AI'}, page_content='LangChain is an AI framework for building LLM-powered applications.'),\n",
       " Document(metadata={'source': 'official_docs', 'category': 'AI'}, page_content='LangChain is useful for building LLM-powered applications and chatbots')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Creating a simple document\n",
    "docs = [Document(\n",
    "    page_content=\"LangChain is an AI framework for building LLM-powered applications.\",\n",
    "    metadata={\"source\": \"official_docs\", \"category\": \"AI\"},),\n",
    "        Document(\n",
    "    page_content=\"LangChain is useful for building LLM-powered applications and chatbots\",\n",
    "    metadata={\"source\": \"official_docs\", \"category\": \"AI\"},)]\n",
    "        \n",
    "        \n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ VectorStore in LangChain\n",
    "\n",
    "A VectorStore is a database that: ‚úÖ Stores documents as numerical vectors\n",
    "‚úÖ Finds similar text using semantic search\n",
    "‚úÖ Works with LLMs (Large Language Models) for AI applications\n",
    "\n",
    "LangChain supports multiple VectorStores, like:\n",
    "\n",
    "- FAISS (Local, Fast & Simple)\n",
    "- Chroma (Open-source, lightweight)\n",
    "- Pinecone (Cloud-based, scalable)\n",
    "- Weaviate, Milvus, Qdrant, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002B82BAF5CA0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002B82BAFD850>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm= ChatGroq(groq_api_key=groq_api_key, model='llama3-8b-8192')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x2b82bb09580>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store=Chroma.from_documents(docs, embedding=embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='da02a7e3-3723-43a5-b9df-cffb2b128d91', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is an AI framework for building LLM-powered applications.'),\n",
       " Document(id='a2cd583e-c698-4501-bb55-3bf754ba718f', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is useful for building LLM-powered applications and chatbots')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='da02a7e3-3723-43a5-b9df-cffb2b128d91', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is an AI framework for building LLM-powered applications.'),\n",
       " Document(id='a2cd583e-c698-4501-bb55-3bf754ba718f', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is useful for building LLM-powered applications and chatbots')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Async Search\n",
    "\n",
    "await vector_store.asimilarity_search(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(id='da02a7e3-3723-43a5-b9df-cffb2b128d91', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is an AI framework for building LLM-powered applications.'),\n",
       "  1.167879581451416),\n",
       " (Document(id='a2cd583e-c698-4501-bb55-3bf754ba718f', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is useful for building LLM-powered applications and chatbots'),\n",
       "  1.6071475744247437)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_score(\"ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "In LangChain, a retriever is a component responsible for fetching relevant documents, data, or information from a source based on a query. It's typically used in natural language processing (NLP) tasks where you need to retrieve useful information from a collection of text (like databases, files, or other data sources) to answer questions or generate responses.\n",
    "\n",
    "Key points about retrievers:\n",
    "- Document Search: Retrievers are used to search through a collection of documents and pull the most relevant ones based on a query.\n",
    "- Context for LLMs: They often work together with Language Models (LLMs) to help provide context or knowledge that the LLM can use to generate more accurate or insightful responses.\n",
    "- Different Sources: The source of information can be anything from a simple file to a database or a complex knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='da02a7e3-3723-43a5-b9df-cffb2b128d91', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is an AI framework for building LLM-powered applications.')],\n",
       " [Document(id='a2cd583e-c698-4501-bb55-3bf754ba718f', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is useful for building LLM-powered applications and chatbots')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retreiver=RunnableLambda(vector_store.similarity_search).bind(k=1)\n",
    "retreiver.batch([\"ai\",\"langchain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore Retriever \n",
    "\n",
    "It is a tool commonly used in AI, machine learning, and natural language processing to search and retrieve information based on vector similarity rather than exact keyword matches.\n",
    "\n",
    "    VectorStore:\n",
    "\n",
    "Think of it as a database where data (like text, images, or documents) is stored as vectors (which are just lists of numbers).\n",
    "These vectors represent the meaning or features of the data, so similar data points have vectors that are close to each other.\n",
    "\n",
    "    Retriever:\n",
    "\n",
    "A retriever is like a search engine. Instead of searching by exact words, it searches by comparing vectors.\n",
    "When you give it a query (like a sentence or question), it converts the query to a vector and finds the vectors in the store that are most similar.\n",
    "\n",
    "    Real-World Example:\n",
    "Imagine you have a collection of movie reviews stored as vectors. If you search for \"funny comedy,\" the retriever will look for reviews whose vectors are close to the meaning of \"funny comedy,\" even if those exact words aren't used in the reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='da02a7e3-3723-43a5-b9df-cffb2b128d91', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is an AI framework for building LLM-powered applications.')],\n",
       " [Document(id='a2cd583e-c698-4501-bb55-3bf754ba718f', metadata={'category': 'AI', 'source': 'official_docs'}, page_content='LangChain is useful for building LLM-powered applications and chatbots')]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "\n",
    "retriever.batch([\"ai\",\"langchain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a useful tool for building applications and chatbots that are powered by Large Language Models (LLMs). According to the official documentation, it provides the necessary framework and resources to create LLM-powered applications and chatbots, making it a valuable resource for developers in the field of AI.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Message Template: Correctly structured prompt\n",
    "messages = \"\"\"\n",
    "You are an assistant. Use the following context to answer the question.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Creating the prompt from the template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", messages)])\n",
    "\n",
    "# Proper Chain Construction: Remove redundant 'llm'\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Retrieve context and pass the question\n",
    "    | prompt  # Apply the prompt to format the query and context\n",
    "    | model   # Use the model (LLM) to generate the answer\n",
    ")\n",
    "\n",
    "# Invoke with a query\n",
    "response = rag_chain.invoke(\"Tell me about LangChain\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
